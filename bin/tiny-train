#!/usr/bin/env python3

import os
import sys
import time

main_path = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, os.path.join(main_path, 'src/python3'))

if len(sys.argv) < 2 or len(sys.argv) > 6:
    print(f'usage: {os.path.basename(sys.argv[0])} <pf-files> [ <cf-files> ] [ <model-dir> ] [ <device> ] [ <batch-size> ]',
          file=sys.stderr)
    sys.exit(1)
PF_FILES = sys.argv[1].split(':')
CF_FILES = sys.argv[2].split(':') if len(sys.argv) > 2 and sys.argv[2] != '-' else None
MODEL_DIR = sys.argv[3] if len(sys.argv) > 3 and sys.argv[3] != '-' else \
        os.path.abspath(os.path.join(main_path, 'run', 'little', '%s-%d' % (
            os.popen('uname -n').read().strip() or 'anonymous', int(time.time()),
        )))
os.makedirs(MODEL_DIR)
DEVICE = sys.argv[4] if len(sys.argv) > 4 and sys.argv[4] != '-' else 'cuda:0'
BATCH_SIZE = int(sys.argv[5]) if len(sys.argv) > 5 and sys.argv[5] != '-' else 128

if CF_FILES is not None and len(PF_FILES) != len(CF_FILES):
    raise ValueError('len(PF_FILES) != len(CF_FILES)')
NUM_CLASS = 5

import little
import adjet
import math
import numpy as np
import torch
import logging

logging.basicConfig(level=logging.INFO)

def split_training_validation(x, validation_ratio):
    num = x.shape[0]
    num_validation = int(math.ceil(num * validation_ratio))
    num_training = num - num_validation
    return x[:num_training], x[num_training:]

# Model, loss function, and optimizer.
if CF_FILES is None:
    model = little.ParticleTransformerTagger(adjet.EVT_LABEL, adjet.JET_LABEL, adjet.NFEAT_LEP, adjet.NFEAT_PHO, num_classes=NUM_CLASS).to(DEVICE)
else:
    model = little.ParticleTransformerTagger(adjet.EVT_LABEL, adjet.NHIDDEN, adjet.NFEAT_LEP, adjet.NFEAT_PHO, num_classes=NUM_CLASS).to(DEVICE)
loss_func = torch.nn.functional.cross_entropy
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.8)

def feature_wise_normalize(x, eps=1e-8):
    mean = np.mean(x, axis=tuple(range(x.ndim - 1)), keepdims=True)
    std = np.std(x, axis=tuple(range(x.ndim - 1)), keepdims=True)
    print('mean:', mean, 'std:', std, sep='\n')
    return (x - mean) / np.hypot(std, eps)

# Data for training and validation.
evt, jet, lep, pho, msk, label = [], [], [], [], [], []
for (i, PF_FILE) in enumerate(PF_FILES):
    logging.info('loading %s' % PF_FILE)
    if CF_FILES is None:
        d = adjet.ADRawDataSet(PF_FILE, PF_FILE[:-3] + '_events.gz', PF_FILE[:-3] + '_leptons.gz', PF_FILE[:-3] + '_photons.gz')
    else:
        d = adjet.ADParTDataSet(CF_FILES[i], PF_FILE[:-3] + '_events.gz', PF_FILE[:-3] + '_leptons.gz', PF_FILE[:-3] + '_photons.gz')
    logging.info('%05d samples loaded from %s' % (d.evt.shape[0], PF_FILE))
    for c, d in (evt, d.evt), (jet, d.jet), (lep, d.lep), (pho, d.pho), (msk, d.msk), (label, d.label): c.append(d)
logging.info('preprocessing data')
evt, jet, lep, pho, msk, label = map(np.concatenate, (evt, jet, lep, pho, msk, label))
index = np.arange(evt.shape[0])
np.random.shuffle(index)
evt, jet, lep, pho, msk, label = map(lambda x: x[index], (evt, jet, lep, pho, msk, label))
evt, jet, lep, pho = map(feature_wise_normalize, (evt, jet, lep, pho))
logging.info('total loaded: num_samples=%05d' % (evt.shape[0]))
training_evt, validation_evt = split_training_validation(evt, 0.2)
training_jet, validation_jet = split_training_validation(jet, 0.2)
training_lep, validation_lep = split_training_validation(lep, 0.2)
training_pho, validation_pho = split_training_validation(pho, 0.2)
training_msk, validation_msk = split_training_validation(msk, 0.2)
training_label, validation_label = split_training_validation(label, 0.2)
logging.info('data ready in main memory: training=%05d validation=%05d' % (training_label.shape[0], validation_label.shape[0]))

def create_tensor(data, dtype): return torch.tensor(data, dtype=dtype, device=DEVICE)
training = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*map(create_tensor, (
    training_evt, training_jet, training_lep, training_pho, training_msk, training_label
),(
    torch.float32, torch.float32, torch.float32, torch.float32, torch.bool, torch.int64
))), BATCH_SIZE)
validation = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*map(create_tensor, (
    validation_evt, validation_jet, validation_lep, validation_pho, validation_msk, validation_label
),(
    torch.float32, torch.float32, torch.float32, torch.float32, torch.bool, torch.int64
))), BATCH_SIZE)
logging.info('data copied to GPU memory successfully')

def format_acc_of_classes(class_acc, class_cnt, acc):
    numbers = np.hstack([
        class_acc.reshape(-1, 1),
        class_cnt.reshape(-1, 1),
    ]).flatten()
    return '%.3f(%.0f) | ' % (acc, class_cnt.sum()) + ' '.join(['%.3f(%.0f)'] * len(class_acc)) % tuple(numbers)

# out, loss, top1, acc
def run_model(evt, jet, lep, pho, msk, label, loss_func):
    evt = evt.permute(0, 2, 1).contiguous()  # (N, C, P)
    jet = jet.permute(0, 2, 1).contiguous()  # (N, C, P)
    lep = lep.permute(0, 2, 1).contiguous()  # (N, C, P)
    pho = pho.permute(0, 2, 1).contiguous()  # (N, C, P)
    out = model(evt, jet, lep, pho, msk)  # (N, C)
    label = label.squeeze(-1)
    loss = loss_func(out, label)
    top1 = out.argmax(-1)
    acc = (top1 == label).to(out.dtype)
    return out, loss, top1, acc

def train_one_epoch(epoch_id):
    lr = scheduler.get_last_lr()[0]
    print('Epoch %03d (learning_rate=%.3g)\n----------------------------------------' % (epoch_id, lr))

    model.train()
    loss = 0.0
    acc = 0.0
    for batch_id, (batch_evt, batch_jet, batch_lep, batch_pho, batch_msk, batch_label) in enumerate(training):
        _, batch_loss, batch_top1, batch_acc = run_model(batch_evt, batch_jet, batch_lep, batch_pho, batch_msk, batch_label, loss_func)
        print('training batch %03d: loss=%.3f acc=%.3f' % (batch_id, batch_loss, batch_acc.mean()))
        batch_loss.backward()
        optimizer.step()
        loss += batch_loss
        acc += batch_acc.sum()
    loss /= len(training)
    acc /= training_evt.shape[0]
    print('Summary of training: loss=%.3f acc=%.3f' % (loss, acc))
    scheduler.step()

    confusion = np.zeros(shape=(NUM_CLASS, NUM_CLASS), dtype='int64')

    model.eval()
    loss = 0.0
    acc = 0.0
    with torch.no_grad():
        for batch_id, (batch_evt, batch_jet, batch_lep, batch_pho, batch_msk, batch_label) in enumerate(validation):
            _, batch_loss, batch_top1, batch_acc = run_model(batch_evt, batch_jet, batch_lep, batch_pho, batch_msk, batch_label, loss_func)
            print('validation batch %03d: loss=%.3f acc=%.3f' % (batch_id, batch_loss, batch_acc.mean()))
            loss += batch_loss
            acc += batch_acc.sum()
            for target, prediction in zip(batch_label.flatten(), batch_top1.flatten()):
                confusion[int(target), int(prediction)] += 1
    loss /= len(validation)
    acc /= validation_evt.shape[0]
    print('Summary of validation: loss=%.3f acc=%.3f' % (loss, acc))

    print('Confusion matrix:', confusion, sep='\n')
    confusion = confusion.astype('float32') / confusion.sum(axis=-1, keepdims=True)
    print('Confusion matrix:', confusion, sep='\n')

for epoch_id in range(50):
    train_one_epoch(epoch_id)
