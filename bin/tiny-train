#!/usr/bin/env python3

import os
import sys
import time

main_path = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, os.path.join(main_path, 'src/python3'))

if len(sys.argv) < 2 or len(sys.argv) > 6:
    print(f'usage: {os.path.basename(sys.argv[0])} <pf-files> [ <cf-files> ] [ <model-dir> ] [ <device> ] [ <batch-size> ]',
          file=sys.stderr)
    sys.exit(1)
PF_FILES = sys.argv[1].split(':')
CF_FILES = sys.argv[2].split(':') if len(sys.argv) > 2 and sys.argv[2] != '-' else None
MODEL_DIR = sys.argv[3] if len(sys.argv) > 3 and sys.argv[3] != '-' else \
        os.path.abspath(os.path.join(main_path, 'run', 'tiny', '%s-%d' % (
            os.popen('uname -n').read().strip() or 'anonymous', int(time.time()),
        )))
os.makedirs(MODEL_DIR)
DEVICE = sys.argv[4] if len(sys.argv) > 4 and sys.argv[4] != '-' else None
BATCH_SIZE = int(sys.argv[5]) if len(sys.argv) > 5 and sys.argv[5] != '-' else 128

if CF_FILES is not None and len(PF_FILES) != len(CF_FILES):
    raise ValueError('len(PF_FILES) != len(CF_FILES)')

import tiny
import adjet
import math
import numpy as np
import torch
import logging

logging.basicConfig(level=logging.INFO)

# Configuration.
if DEVICE is not None: tiny.device = DEVICE

def split_training_validation(x, validation_ratio):
    num = x.shape[0]
    num_validation = int(math.ceil(num * validation_ratio))
    num_training = num - num_validation
    return x[:num_training], x[num_training:]

# Model, loss function, and optimizer.
if CF_FILES is None:
    model = tiny.TinyEventClassifier(adjet.EVT_LABEL, adjet.JET_LABEL, adjet.NFEAT_LEP, adjet.NFEAT_PHO, classifier=tiny.TinyClassifier, num_classes=5)
else:
    model = tiny.TinyEventClassifier(adjet.EVT_LABEL, adjet.NHIDDEN, adjet.NFEAT_LEP, adjet.NFEAT_PHO, classifier=tiny.TinyClassifier, num_classes=5)
loss_func = torch.nn.functional.cross_entropy
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.8)

def feature_wise_normalize(x, eps=1e-8):
    mean = np.mean(x, axis=tuple(range(x.ndim - 1)), keepdims=True)
    std = np.std(x, axis=tuple(range(x.ndim - 1)), keepdims=True)
    print('mean:', mean, 'std:', std, sep='\n')
    return (x - mean) / np.hypot(std, eps)

# Data for training and validation.
evt, jet, lep, pho, label = [], [], [], [], []
for (i, PF_FILE) in enumerate(PF_FILES):
    logging.info('loading %s' % PF_FILE)
    if CF_FILES is None:
        d = adjet.ADRawDataSet(PF_FILE, PF_FILE[:-3] + '_events.gz', PF_FILE[:-3] + '_leptons.gz', PF_FILE[:-3] + '_photons.gz')
    else:
        d = adjet.ADParTDataSet(CF_FILES[i], PF_FILE[:-3] + '_events.gz', PF_FILE[:-3] + '_leptons.gz', PF_FILE[:-3] + '_photons.gz')
    e, j, l, p, lab = map(lambda x: x[:50000], (d.evt, d.jet, d.lep, d.pho, d.label))
    logging.info('%05d samples loaded from %s' % (e.shape[0], PF_FILE))
    for c, d in (evt, e), (jet, j), (lep, l), (pho, p), (label, lab): c.append(d)
logging.info('preprocessing data')
evt, jet, lep, pho, label = map(np.concatenate, (evt, jet, lep, pho, label))
index = np.arange(evt.shape[0])
np.random.shuffle(index)
evt, jet, lep, pho, label = map(lambda x: x[index], (evt, jet, lep, pho, label))
evt, jet, lep, pho = map(feature_wise_normalize, (evt, jet, lep, pho))
label = torch.nn.functional.one_hot(torch.tensor(label.squeeze(), dtype=torch.int64)).numpy()
logging.info('total loaded: num_samples=%05d' % (evt.shape[0]))
training_evt, validation_evt = split_training_validation(evt, 0.2)
training_jet, validation_jet = split_training_validation(jet, 0.2)
training_lep, validation_lep = split_training_validation(lep, 0.2)
training_pho, validation_pho = split_training_validation(pho, 0.2)
training_label, validation_label = split_training_validation(label, 0.2)
logging.info('data ready in main memory: training=%05d validation=%05d' % (training_label.shape[0], validation_label.shape[0]))

def create_tensor(data): return torch.tensor(data, dtype=torch.float32, device=tiny.device)
training = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*map(create_tensor, (
    training_evt, training_jet, training_lep, training_pho, training_label
))), BATCH_SIZE)
validation = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*map(create_tensor, (
    validation_evt, validation_jet, validation_lep, validation_pho, validation_label
))), BATCH_SIZE)
logging.info('data copied to GPU memory successfully')

def format_acc_of_classes(class_acc, class_cnt, acc):
    numbers = np.hstack([
        class_acc.reshape(-1, 1),
        class_cnt.reshape(-1, 1),
    ]).flatten()
    return '%.3f(%.0f) | ' % (acc, class_cnt.sum()) + ' '.join(['%.3f(%.0f)'] * len(class_acc)) % tuple(numbers)

def train_one_epoch(epoch_id):
    lr = scheduler.get_last_lr()[0]
    print('Epoch %03d (learning_rate=%.3g)\n----------------------------------------' % (epoch_id, lr))

    model.train()
    loss = 0.0
    acc = 0.0
    for batch_id, (batch_evt, batch_jet, batch_lep, batch_pho, batch_label) in enumerate(training):
        _, batch_loss, batch_top1, batch_acc = model.run(batch_evt, batch_jet, batch_lep, batch_pho, batch_label, loss_func)
        print('training batch %03d: loss=%.3f acc=%.3f' % (batch_id, batch_loss, batch_acc.mean()))
        batch_loss.backward()
        optimizer.step()
        loss += batch_loss
        acc += batch_acc.sum()
    loss /= len(training)
    acc /= training_evt.shape[0]
    print('Summary of training: loss=%.3f acc=%.3f' % (loss, acc))
    scheduler.step()

    model.eval()
    with torch.no_grad():
        for batch_id, (batch_evt, batch_jet, batch_lep, batch_pho, batch_label) in enumerate(validation):
            _, batch_loss, batch_top1, batch_acc = model.run(batch_evt, batch_jet, batch_lep, batch_pho, batch_label, loss_func)
            print('validation batch %03d: loss=%.3f acc=%.3f' % (batch_id, batch_loss, batch_acc.mean()))
            loss += batch_loss
            acc += batch_acc.sum()
    loss /= len(validation)
    acc /= validation_evt.shape[0]
    print('Summary of validation: loss=%.3f acc=%.3f' % (loss, acc))

for epoch_id in range(50):
    train_one_epoch(epoch_id)
