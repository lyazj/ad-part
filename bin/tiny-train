#!/usr/bin/env python3

import os
import sys
import time

main_path = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, os.path.join(main_path, 'src/python3'))

if len(sys.argv) < 2 or len(sys.argv) > 5:
    print(f'usage: {os.path.basename(sys.argv[0])} <pf-files> [ <cf-files> ] [ <model-dir> ] [ <device> ]', file=sys.stderr)
    sys.exit(1)
PF_FILES = sys.argv[1].split(':')
CF_FILES = sys.argv[2].split(':') if len(sys.argv) > 2 and sys.argv[2] != '-' else None
MODEL_DIR = sys.argv[3] if len(sys.argv) > 3 and sys.argv[3] != '-' else \
        os.path.abspath(os.path.join(main_path, 'run', 'tiny', '%s-%d' % (
            os.popen('uname -n').read().strip() or 'anonymous', int(time.time()),
        )))
os.makedirs(MODEL_DIR)
DEVICE = sys.argv[4] if len(sys.argv) > 4 and sys.argv[4] != '-' else None

# [TODO] Support data after ParT.
if CF_FILES is not None:
    raise NotImplemented('expect not CF_FILES yet')

import tiny
import adjet
import math
import numpy as np
import torch
import logging
import functools

logging.basicConfig(level=logging.INFO)

# Configuration.
if DEVICE is not None: tiny.device = DEVICE
create_tensor = functools.partial(torch.tensor, dtype=torch.float32, device=tiny.device)
batch_size = 128

def split_training_validation(x, validation_ratio):
    num = x.shape[0]
    num_validation = int(math.ceil(num * validation_ratio))
    num_training = num - num_validation
    return x[:num_training], x[num_training:]

# Model, loss function, and optimizer.
model = tiny.TinyBinaryClassifier(adjet.NFEAT_JET - 1)  # without label
loss_func = torch.nn.functional.binary_cross_entropy
optimizer = torch.optim.Adam(model.parameters())

# Data for training and validation.
data = []
for (i, PF_FILE) in enumerate(PF_FILES):
    logging.info('loading %s' % PF_FILE)
    d = adjet.load_pf(PF_FILE).data[:,:adjet.NFEAT_JET]
    logging.info('%05d samples loaded from %s' % (d.shape[0], PF_FILE))
    data.append(d)
logging.info('combining and shuffling data')
data = np.concatenate(data)
np.random.shuffle(data)
logging.info('total loaded: num_samples=%05d' % (data.shape[0]))
training, validation = split_training_validation(data, 0.2)
logging.info('data ready in main memory: training=%05d validation=%05d' % (training.shape[0], validation.shape[0]))
training_input, training_label = training[:,:-1], training[:,-1:].astype('bool')
training_input, training_label = map(create_tensor, (training_input, training_label))
validation_input, validation_label = validation[:,:-1], validation[:,-1:].astype('bool')
validation_input, validation_label = map(create_tensor, (validation_input, validation_label))
logging.info('data copied to GPU memory successfully')
num_training_batches = (training_input.shape[0] + batch_size - 1) // batch_size
num_validation_batches = (validation_input.shape[0] + batch_size - 1) // batch_size

def get_batch(batch_id, x):
    return x[batch_id * batch_size : (batch_id + 1) * batch_size]

def train_one_epoch(epoch_id):
    print('Epoch %03d\n----------------------------------------' % epoch_id)
    optimizer.zero_grad()
    training_loss, training_acc = 0.0, 0.0
    validation_loss, validation_acc = 0.0, 0.0
    
    model.train()
    for batch_id in range(num_training_batches):
        batch_input, batch_label = map(functools.partial(get_batch, batch_id), (training_input, training_label))
        batch_output, batch_loss, batch_acc = model.run(batch_input, batch_label, loss_func)
        print('training batch %03d: loss=%.3f acc=%.3f' % (batch_id, batch_loss, batch_acc))
        training_loss += batch_loss * batch_output.shape[0]
        training_acc += batch_acc * batch_output.shape[0]
        batch_loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        for batch_id in range(num_validation_batches):
            batch_input, batch_label = map(functools.partial(get_batch, batch_id), (validation_input, validation_label))
            batch_output, batch_loss, batch_acc = model.run(batch_input, batch_label, loss_func)
            print('validation batch %03d: loss=%.3f acc=%.3f' % (batch_id, batch_loss, batch_acc))
            validation_loss += batch_loss * batch_output.shape[0]
            validation_acc += batch_acc * batch_output.shape[0]

    training_loss, training_acc = map(lambda x: x / training_input.shape[0], (training_loss, training_acc))
    validation_loss, validation_acc = map(lambda x: x / validation_input.shape[0], (validation_loss, validation_acc))
    print('Epoch %03d: training_loss=%.3f training_acc=%.3f validation_loss=%.3f validation_acc=%.3f'
          % (epoch_id, training_loss, training_acc, validation_loss, validation_acc), end='\n\n')
    torch.save(model.state_dict(), os.path.join(MODEL_DIR, '%03d' % epoch_id))

for epoch_id in range(100):
    train_one_epoch(epoch_id)
